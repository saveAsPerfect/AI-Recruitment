{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# AI Recruiting Agent v2 — Architecture & Demo\n\n**Stack:** Elasticsearch · LangChain · OpenAI · sentence-transformers · FastAPI · Streamlit"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. New Data Model\n\nSlimmer, focused schemas:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Candidate\ncandidate_example = {\n    'id': 'uuid',\n    'name': 'Ivan Petrov',\n    'email': 'ivan@example.com',\n    'role': 'Senior Python Developer',        # job title\n    'skills': ['python', 'fastapi', 'docker'],\n    'education': 'Computer Science',          # specialty only\n    'experience': '5 years backend at TechCorp, built FastAPI microservices...',\n}\n\n# Vacancy\nvacancy_example = {\n    'id': 'uuid',\n    'title': 'Senior Python Developer',\n    'role': 'Backend Developer',\n    'required_skills': ['python', 'fastapi', 'docker'],\n    'required_education': 'Computer Science',\n    'description': 'We need a backend developer to build scalable REST APIs...',\n}\n\nprint('Schemas defined')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. LangChain Resume Parser\n\nInstead of regex, we use **LangChain + OpenAI** for structured JSON extraction."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "PARSE_SYSTEM = '''You are an expert HR data extractor.\nExtract fields from the resume and return ONLY valid JSON:\n- name, email, role, skills (list), education (specialty), experience (summary)\nDo NOT invent data not present in the resume.'''\n\n# Example LangChain chain (pseudocode):\nfrom_chain = '''\nllm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\nparser = JsonOutputParser(pydantic_object=ParsedCandidate)\nchain = ChatPromptTemplate.from_messages([...]) | llm | parser\nresult = chain.invoke({'resume': raw_text})\n'''\n\n# Simulated result:\nresult = {\n    'name': 'Ivan Petrov',\n    'email': 'ivan@example.com',\n    'role': 'Senior Backend Developer',\n    'skills': ['python', 'fastapi', 'postgresql', 'docker', 'kubernetes', 'aws'],\n    'education': 'Computer Science',\n    'experience': '5 years backend. TechCorp 2020-2024: FastAPI microservices, PostgreSQL. StartupXYZ 2019-2020.'\n}\nimport json\nprint(json.dumps(result, indent=2))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Elasticsearch Index Design\n\nCandidates and vacancies stored with **dense_vector** field for semantic search."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "CANDIDATE_MAPPING = {\n    'mappings': {\n        'properties': {\n            'id':         {'type': 'keyword'},\n            'name':       {'type': 'text'},\n            'role':       {'type': 'text'},\n            'skills':     {'type': 'keyword'},      # exact match + BM25\n            'education':  {'type': 'text'},\n            'experience': {'type': 'text'},\n            'embedding':  {\n                'type': 'dense_vector',\n                'dims': 384,                        # all-MiniLM-L6-v2\n                'index': True,\n                'similarity': 'cosine',\n            },\n        }\n    }\n}\n\nLLM_CACHE_MAPPING = {\n    'mappings': {\n        'properties': {\n            'vacancy_id':   {'type': 'keyword'},\n            'candidate_id': {'type': 'keyword'},\n            'score':        {'type': 'float'},\n            'explanation':  {'type': 'text'},\n            'updated_at':   {'type': 'date'},\n        }\n    }\n}\nprint('Mappings defined')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. BM25 — Baseline\n\nElasticsearch `multi_match` with field boosting. Fastest, no ML required."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "BM25_QUERY = {\n    'query': {\n        'bool': {\n            'should': [\n                {\n                    'multi_match': {\n                        'query': 'python fastapi docker postgresql microservices',\n                        'fields': [\n                            'experience^2',     # experience is most important\n                            'skills^3',         # skills highest boost\n                            'role^2',\n                            'education',\n                            'raw_text',\n                        ],\n                        'fuzziness': 'AUTO',\n                    }\n                },\n                {\n                    'terms': {\n                        'skills': ['python', 'fastapi', 'docker'],\n                        'boost': 4.0                               # exact skill match\n                    }\n                }\n            ]\n        }\n    },\n    'size': 20,\n}\nprint('BM25 query built')\nprint(f'Top-20 candidates fetched for LLM pre-filter')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Semantic — Dense Vector KNN\n\nEmbedding model: `sentence-transformers/all-MiniLM-L6-v2` (384-dim)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import numpy as np\n\n# Model encodes text to 384-dim normalized vector\n# from sentence_transformers import SentenceTransformer\n# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n\n# ES KNN query\nKNN_QUERY = {\n    'knn': {\n        'field': 'embedding',\n        'query_vector': [0.12, -0.05, ...],  # vacancy embedding\n        'k': 10,\n        'num_candidates': 100,\n    }\n}\n\n# Simulate cosine similarity ranking\ncandidates = ['Ivan Petrov', 'Maria Smirnova', 'Alex Kozlov']\n# In reality: vacancy_emb @ candidate_emb (normalized = cosine sim)\nfake_sims = [0.81, 0.42, 0.21]\nfor name, sim in sorted(zip(candidates, fake_sims), key=lambda x: -x[1]):\n    print(f'{name:20s}: cosine_sim = {sim:.4f}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. LLM Scoring — with Cache\n\nBM25 top-20 → GPT-4o-mini → structured JSON with explanation. Results cached in ES."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import json\n\nSYSTEM_PROMPT = '''You are a senior technical recruiter AI.\nScore the candidate fit 0.0-1.0 and return JSON:\n  {\"score\": float, \"matched_skills\": [], \"missing_skills\": [], \"explanation\": \"...\"}'''\n\n# Cache lookup before calling OpenAI:\n# cache_id = f'{vacancy_id}_{candidate_id}'\n# if cached := await es.get(index='llm_cache', id=cache_id): return cached\n\n# Simulated LLM response\nllm_response = {\n    'score': 0.92,\n    'matched_skills': ['python', 'fastapi', 'postgresql', 'docker'],\n    'missing_skills': [],\n    'explanation': 'The candidate has all required technical skills with 5 years of relevant experience. FastAPI and Docker expertise directly match the vacancy requirements.'\n}\nprint(json.dumps(llm_response, indent=2))\n\n# After scoring, write to cache:\n# await es.index(index='llm_cache', id=cache_id, document={**llm_response, 'vacancy_id': ..., 'candidate_id': ..., 'updated_at': ...})",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Hybrid — BM25 + Dense + RRF + Cosine Rerank + LLM\n\nFull pipeline for maximum quality."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ── RRF (Reciprocal Rank Fusion) ──────────────────────────────────────────\ndef rrf_fusion(bm25_hits, dense_hits, k=60):\n    scores = {}\n    meta = {}\n    for rank, hit in enumerate(bm25_hits, 1):\n        scores[hit['id']] = scores.get(hit['id'], 0) + 1/(k + rank)\n        meta[hit['id']] = hit\n    for rank, hit in enumerate(dense_hits, 1):\n        scores[hit['id']] = scores.get(hit['id'], 0) + 1/(k + rank)\n        if hit['id'] not in meta: meta[hit['id']] = hit\n    return sorted([{**meta[id], '_rrf': s} for id, s in scores.items()], key=lambda x: -x['_rrf'])\n\n# ── Simulate pipeline ─────────────────────────────────────────────────────\nbm25 = [{'id':'c1','name':'Ivan','_bm25': 0.9}, {'id':'c2','name':'Maria','_bm25':0.4}, {'id':'c3','name':'Alex','_bm25':0.2}]\ndense = [{'id':'c1','name':'Ivan','_sem': 0.81}, {'id':'c2','name':'Maria','_sem':0.45}, {'id':'c3','name':'Alex','_sem':0.18}]\n\nfused = rrf_fusion(bm25, dense)\nprint('=== After BM25 + Dense + RRF ===')\nfor h in fused:\n    print(f\"{h['name']:20s}: RRF={h['_rrf']:.4f}\")\n\nprint()\nprint('Next step: cosine rerank (re-score with stored embeddings)')\nprint('Then: LLM score top candidates (with cache)')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Pipeline Summary\n\n```\nVacancy text\n    │\n    ├─► BM25 (ES multi_match) ──────────────────┐\n    │                                             ├─► RRF Fusion\n    ├─► Dense KNN (cosine sim, ES knn query) ────┘\n    │                                             │\n    │                              Cosine rerank (embedding dot product)\n    │                                             │\n    └─► LLM scoring (GPT-4o-mini) ◄──────────────┘\n            │   ▲\n            │   │ cache miss\n            ▼   │\n        ES llm_cache ──► cache hit (instant return)\n```\n\n**LLM Cache schema:**\n```\nllm_cache index:\n  vacancy_id   | keyword\n  candidate_id | keyword  \n  score        | float\n  explanation  | text\n  updated_at   | date\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Quick Start\n\n```bash\n# 1. Set API key\necho 'OPENAI_API_KEY=sk-...' > .env\n\n# 2. Start everything\ndocker compose up --build\n\n# 3. Seed sample data\ndocker compose exec api python scripts/seed_data.py\n\n# 4. Open\n# Swagger:   http://localhost:8000/docs\n# Streamlit: http://localhost:8501\n# ES:        http://localhost:9200\n\n# 5. Test API\ncurl 'http://localhost:8000/api/v1/recommendations?job_id=<id>&method=hybrid&top_k=5'\n```"
  }
 ]
}