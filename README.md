# ğŸ¤– AI Recruiting Agent v2

**Stack:** Elasticsearch Â· LangChain Â· OpenAI Â· sentence-transformers Â· FastAPI Â· Streamlit Â· Docker

---

## Data Model

```python
# Candidate
{
  id, name, email,
  role,              # "Senior Python Developer"
  skills,            # ["python", "fastapi", "docker"]
  education,         # "Computer Science"  (specialty only)
  experience,        # free-form text about work history
}

# Vacancy
{
  id, title, role,
  required_skills,       # ["python", "fastapi", "docker"]
  required_education,    # "Computer Science"
  description,           # full job description
}
```

---

## Matching Methods

| Method | Description | When to use |
|--------|-------------|-------------|
| **BM25** | ES `multi_match` with field boosts | Fast baseline, large pools |
| **Semantic** | KNN dense vector cosine similarity | Concept/synonym matching |
| **LLM** | BM25(top-20) â†’ GPT-4o-mini scoring | High accuracy, small pool |
| **Hybrid** | BM25 + Dense â†’ RRF â†’ cosine rerank â†’ LLM | Best overall quality |

### Hybrid Pipeline

```
Vacancy
  â”œâ”€â–º BM25 (top-20) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                    â”œâ”€â–º RRF Fusion
  â”œâ”€â–º Dense KNN (top-20) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚                          â”‚
  â”‚               Cosine Rerank (stored embeddings)
  â”‚                          â”‚
  â””â”€â–º LLM (GPT-4o-mini) â—„â”€â”€â”€â”˜  cached in ES
```

### LLM Cache

`llm_cache` index stores `(vacancy_id, candidate_id) â†’ score, explanation, updated_at`.  
Cache is checked before every LLM call â€” no redundant API costs.

---

## Quick Start

```bash
# 1. Configure
cp .env.example .env
# Edit .env â†’ add OPENAI_API_KEY

# 2. Start
docker compose up --build

# 3. Seed sample data
docker compose exec api python scripts/seed_data.py

# 4. Access
# Swagger:   http://localhost:8000/docs
# Streamlit: http://localhost:8501
# ES:        http://localhost:9200
```

---

## API Endpoints

```http
GET  /api/v1/recommendations?job_id=<id>&method=hybrid&top_k=5
POST /api/v1/candidates/upload         â€” PDF/DOCX/TXT â†’ LangChain parse â†’ ES
POST /api/v1/candidates                â€” manual candidate creation
GET  /api/v1/candidates
POST /api/v1/vacancies
GET  /api/v1/vacancies
GET  /api/v1/health
```

---

## Project Structure

```
ai-recruiting-agent/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                  # FastAPI app + ES lifespan
â”‚   â”œâ”€â”€ api/routes.py            # REST endpoints
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py            # Settings via env vars
â”‚   â”‚   â”œâ”€â”€ elasticsearch.py     # ES client + index mappings
â”‚   â”‚   â”œâ”€â”€ storage.py           # ES CRUD (candidates, vacancies, cache)
â”‚   â”‚   â”œâ”€â”€ embeddings.py        # Singleton embedding service
â”‚   â”‚   â””â”€â”€ matching.py          # BM25 Â· Semantic Â· LLM Â· Hybrid + RRF
â”‚   â”œâ”€â”€ models/schemas.py        # Pydantic schemas
â”‚   â””â”€â”€ utils/resume_parser.py   # LangChain + OpenAI structured parsing
â”œâ”€â”€ streamlit_app.py             # Frontend
â”œâ”€â”€ scripts/seed_data.py         # Sample data loader
â”œâ”€â”€ notebooks/                   # Architecture + demo notebook
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Dockerfile.streamlit
â”œâ”€â”€ docker-compose.yml           # ES + API + Frontend
â””â”€â”€ .env.example
```
